{\n  "attention_probs_dropout_prob": 0.1,\n  "backward_compatible": true,\n  "hidden_act": "gelu",\n  "hidden_dropout_prob": 0.1,\n  "hidden_size": 1024,\n  "initializer_range": 0.02,\n  "intermediate_size": 4096,\n  "max_position_embeddings": 512,\n  "num_attention_heads": 16,\n  "num_hidden_layers": 24,\n  "type_vocab_size": 2,\n  "vocab_size": 30522\n}\n'